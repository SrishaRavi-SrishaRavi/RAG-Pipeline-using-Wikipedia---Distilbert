{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvDahPJEHqml9CA69KalfO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrishaRavi-SrishaRavi/RAG-Pipeline-using-Wikipedia---Distilbert/blob/main/Building_RAG_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "573N3ZbABFoc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets import all the libraries required"
      ],
      "metadata": {
        "id": "M6iXBZ4yCMTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **## Step 1: Retrieving Knowledge from Wikipedia**"
      ],
      "metadata": {
        "id": "rNVqI82rCSn_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-AD89JYo69b"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def get_wikipedia_content(topic):\n",
        "  try:\n",
        "    page = wikipedia.page(topic)\n",
        "    return page.content\n",
        "  except wikipedia.exceptions.PageError:\n",
        "    return None\n",
        "  except wikipedia.exceptions.DisambiguationError as e:\n",
        "    #handle cases where the topic is ambigious\n",
        "    print(f\"Ambiguous topic, please be more specific. Options:{e.options}\")\n",
        "    return None\n",
        "\n",
        "topic = input(\"Enter a topic you want to learn about:\")\n",
        "document = get_wikipedia_content(topic)\n",
        "\n",
        "if not document:\n",
        "  print(\"Sorry, I am not able to retrieve the particular document, please check your spelling once\")\n",
        "  exit()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "def split_text(text,chunk_size=256,chunk_overlap =20):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  chunks = []\n",
        "  start =0\n",
        "  while start < len(tokens):\n",
        "    end = min(start + chunk_size,len(tokens))\n",
        "    chunks.append(tokenizer.convert_tokens_to_string(tokens[start:end]))\n",
        "    if end ==len(tokens):\n",
        "      break\n",
        "    start = end - chunk_overlap\n",
        "  return chunks\n",
        "\n",
        "chunks = split_text(document)\n",
        "print(f\"Number of chunks:{len(chunks)}\")\n",
        "\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "embeddings = embedding_model.encode(chunks)\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "\n",
        "query = input(\"Ask a question about this topic\")\n",
        "query_embeddings = embedding_model.encode([query])\n",
        "\n",
        "k=3\n",
        "distances,indices = index.search(np.array(query_embeddings),k)\n",
        "retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "print(\"Retrieved Chunks\")\n",
        "for chunk in retrieved_chunks:\n",
        "  print(\"- \"+ chunk)\n",
        "\n",
        "\n",
        "qa_model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "qa_pipeline = pipeline(\"question-answering\",model=qa_model,tokenizer=qa_tokenizer)\n",
        "\n",
        "context = \" \".join(retrieved_chunks)\n",
        "answer = qa_pipeline(question=query,context=context)\n",
        "print(f\"Answer:{answer['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# ðŸ”§ Replace this with your notebook's filename\n",
        "notebook_path = \"/content/Building RAG Pipeline.ipynb\"\n",
        "\n",
        "# Load and clean\n",
        "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Remove bad widget metadata if exists\n",
        "if \"widgets\" in nb.metadata:\n",
        "    del nb.metadata[\"widgets\"]\n",
        "\n",
        "# Save cleaned version (overwrite same file)\n",
        "with open(notebook_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(f\"âœ… Cleaned notebook: {notebook_path}\")\n"
      ],
      "metadata": {
        "id": "luYcxPVYXSBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving Wikipedia content based on user provided topic using the Wikipedia API.If the topic is valid, the function returns the page content; otherwise, it handles error by either notifiying the user a ambiuous topic with multiple options or exiting if no relevant page found."
      ],
      "metadata": {
        "id": "qP5Fy9lgDmq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wikipedia articles are long, splitting them into smaller overlapping chunks for better retrievel"
      ],
      "metadata": {
        "id": "kwCgxAM9EE_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the retrieved content from wikipedia and splitting it up into small overlapping chunks for efficient retrieval.\n",
        "Pre-trainedTokenizer = all-mpnet-base-v2\n",
        "chunk_szie = 256\n",
        "overlap tokens= 20\n"
      ],
      "metadata": {
        "id": "WX9cJdYtPJie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Storing and Retrieving Knowledge**"
      ],
      "metadata": {
        "id": "Pixr0GPcPik5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting text chunks into numerical embeddings using the Sentence Transformer model (all-mpnet-base-v2), which captures their semantic meaning.\n",
        "Creating a FAISS index with an L2 (Euclidean) distance metric and store\n",
        "the embeddings in it. This will allow us to efficiently retrieve the most relevant chunks based on a userâ€™s query."
      ],
      "metadata": {
        "id": "7-9At8LSQrpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Querying the RAG Pipeline**"
      ],
      "metadata": {
        "id": "X0zORnUUQgWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Convert the query into an embedding.\n",
        "*  Retrieve the top-k most relevant chunks using FAISS.\n",
        "*  Use an LLM-powered question-answering model to generate the answer.\n"
      ],
      "metadata": {
        "id": "mKUr5czrRk4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Answering the Question with an LLM**"
      ],
      "metadata": {
        "id": "uQRuJFdkSCWw"
      }
    }
  ]
}